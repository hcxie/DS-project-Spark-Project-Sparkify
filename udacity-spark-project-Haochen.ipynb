{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Capstone Porject workspace for full data set(12GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf, isnull\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "import datetime\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, DecisionTreeClassifier, NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data\n",
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read in full sparkify dataset\n",
    "#event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "path = \"mini_sparkify_event_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark_df = spark.read.json(event_data)\n",
    "spark_df = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean null and empty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df_clean=spark_df.filter(spark_df[\"userId\"]!=\"\")\n",
    "spark_df_clean=spark_df_clean.dropna(how=\"any\",subset=[\"userId\",'sessionId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert ts to real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_time = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "spark_df_clean = spark_df_clean.withColumn(\"time\", gen_time(spark_df_clean['ts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get hour, weekday and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_hour = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).hour)\n",
    "spark_df_clean = spark_df_clean.withColumn(\"hour\", gen_hour(spark_df_clean['ts']))\n",
    "\n",
    "gen_weekday = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%w\"))\n",
    "spark_df_clean = spark_df_clean.withColumn(\"weekday\", gen_weekday(spark_df_clean['ts']))\n",
    "\n",
    "gen_day = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).day)\n",
    "spark_df_clean = spark_df_clean.withColumn(\"day\", gen_day(spark_df_clean['ts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert location to state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_state=udf(lambda x:x[-2:])\n",
    "spark_df_clean = spark_df_clean.withColumn(\"location_state\", get_state(spark_df_clean['location']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simplify userAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_useragent=udf(lambda x:\"\".join(x[x.index('(')+1:x.index(')')]))\n",
    "spark_df_clean= spark_df_clean.withColumn(\"sim_user_agent\", simp_useragent(spark_df_clean['userAgent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_merge(df1, df2):\n",
    "    \"\"\"\n",
    "    This function is used to merge the feature using left join\n",
    "    input: two data frame to be merged\n",
    "    output: merged dataframe\n",
    "    \"\"\"\n",
    "    df2 = df2.withColumnRenamed(\"userId\", \"userIdTemp\")\n",
    "    df = df1.join(df2, df1.userId == df2.userIdTemp, \"left\").drop(\"userIdTemp\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_list\n",
    "df_feature=spark_df_clean.select('userId').dropDuplicates().sort('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = [(row['userId']) for row in spark_df_clean.select(\"userId\").dropDuplicates().sort('userId').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender\n",
    "gender_df=spark_df_clean.select('userId','gender').dropDuplicates().sort('userId')\n",
    "df_feature=features_merge(df_feature,gender_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level\n",
    "level_df=spark_df_clean.select('userId','level').dropDuplicates().sort('userId')\n",
    "df_feature=features_merge(df_feature,level_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "method_df=spark_df_clean.select('userId','method').dropDuplicates().sort('userId')\n",
    "df_feature=features_merge(df_feature,method_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location_state\n",
    "location_state_df=spark_df_clean.select('userId','location_state').dropDuplicates().sort('userId')\n",
    "df_feature=features_merge(df_feature,location_state_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user agent\n",
    "agent_df=spark_df_clean.select('userId','sim_user_agent').dropDuplicates().sort('userId')\n",
    "df_feature=features_merge(df_feature,agent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get count usage of total page \n",
    "page_list = [(row['page']) for row in spark_df_clean.select(\"page\").dropDuplicates().collect()]\n",
    "page_view_total_count=spark_df_clean.groupby(\"userId\").count().sort('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get count usage of each page type\n",
    "temp_df_feature=spark_df_clean.select('userId').dropDuplicates().sort('userId')\n",
    "for page in page_list:\n",
    "    \n",
    "    col_name = \"count_\" + page.replace(\" \", \"\")\n",
    "    temp_page_count=spark_df_clean.filter(spark_df_clean['page']==page).groupby(\"userId\").count().sort('userId')\n",
    "    temp_page_count=temp_page_count.withColumnRenamed(\"count\",col_name)\n",
    "    \n",
    "    temp_page_mints=spark_df_clean.filter(spark_df_clean['page']==page).groupby(\"userId\").min('ts').sort('userId')\n",
    "    temp_page_maxts=spark_df_clean.filter(spark_df_clean['page']==page).groupby(\"userId\").max('ts').sort('userId')\n",
    "    temp_page_minmaxts=features_merge(temp_page_maxts,temp_page_mints)\n",
    "    get_minus=udf(lambda x,y:(x-y)/1000/60)\n",
    "    temp_page_minmaxts = temp_page_minmaxts.withColumn(\"totalts\", get_minus(temp_page_minmaxts['max(ts)'],temp_page_minmaxts['min(ts)']))\n",
    "    temp_page_minmaxts=temp_page_minmaxts.drop('max(ts)','min(ts)')\n",
    "    col_name_2=\"totalts_\" + page.replace(\" \", \"\")\n",
    "    temp_page_minmaxts=temp_page_minmaxts.withColumnRenamed(\"totalts\",col_name_2)\n",
    "    \n",
    "    \n",
    "    temp_df_feature=features_merge(temp_df_feature,temp_page_count)\n",
    "    temp_df_feature=features_merge(temp_df_feature,temp_page_minmaxts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_feature=features_merge(temp_df_feature,page_view_total_count)\n",
    "get_div=udf(lambda x,y:x/y if y>0 else 0)\n",
    "for page in page_list:\n",
    "    col_name = \"count_\" + page.replace(\" \", \"\")\n",
    "    col_name_2=\"totalts_\" + page.replace(\" \", \"\")\n",
    "    col_name_3=\"percentage_\" + page.replace(\" \", \"\")\n",
    "    col_name_4=\"freq_\" + page.replace(\" \", \"\")\n",
    "    temp_df_feature.withColumn(col_name_3, get_div(temp_df_feature[col_name],temp_df_feature['count']))\n",
    "    temp_df_feature.withColumn(col_name_4, get_div(temp_df_feature[col_name],temp_df_feature[col_name_2]))\n",
    "    temp_df_feature=temp_df_feature.drop(col_name,col_name_2)\n",
    "\n",
    "temp_df_feature=temp_df_feature.drop('count')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_feature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature=features_merge(df_feature,temp_df_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of days after registration\n",
    "user_max_time = spark_df_clean.groupby(\"userId\").max(\"ts\").sort(\"userId\")\n",
    "user_reg_time = spark_df_clean.select(\"userId\", \"registration\").dropDuplicates().sort(\"userId\")\n",
    "user_max_time=features_merge(user_max_time,user_reg_time)\n",
    "\n",
    "get_minus1=udf(lambda x,y:(x-y)/1000/60/60/24)\n",
    "user_max_time.withColumn(\"number_of_days\", get_minus1(user_max_time['max(ts)'],user_max_time['registration']))\n",
    "user_max_time=user_max_time.drop('max(ts)','registration')\n",
    "df_feature=features_merge(df_feature,user_max_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session\n",
    "#get total session time of each session\n",
    "sessiontime_sparkdf=spark_df_clean.groupby(\"userId\",'sessionId').agg(((max(spark_df_clean['ts'])-min(spark_df_clean['ts']))/1000/60).alias('sessiontime')).sort('userId','sessionId')\n",
    "# get how many session per user has\n",
    "session_count=sessiontime_sparkdf.groupby(\"userId\").count().sort('userId')\n",
    "session_count=session_count.withColumnRenamed(\"count\",\"session_count\")\n",
    "\n",
    "# get max session time for each user\n",
    "max_session_t=sessiontime_sparkdf.groupby(\"userId\").max('sessiontime').sort('userId')\n",
    "# get min session time for each user\n",
    "min_session_t=sessiontime_sparkdf.groupby(\"userId\").min('sessiontime').sort('userId')\n",
    "# get average session time for each user\n",
    "avg_session_t=sessiontime_sparkdf.groupby(\"userId\").avg('sessiontime').sort('userId')\n",
    "\n",
    "\n",
    "df_feature=features_merge(df_feature,session_count)\n",
    "df_feature=features_merge(df_feature,max_session_t)\n",
    "df_feature=features_merge(df_feature,min_session_t)\n",
    "df_feature=features_merge(df_feature,avg_session_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song\n",
    "# how many songs each user played per session\n",
    "song_sparkdf=spark_df_clean.filter(spark_df_clean['page']==\"NextSong\").groupby(\"userId\",'sessionId').count().sort('userId','sessionId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of songs per session\n",
    "max_song_session=song_sparkdf.groupby(\"userId\").max('count').sort('userId')\n",
    "max_song_session=max_song_session.withColumnRenamed(\"max(count)\",\"session_max_num_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min number of songs per session\n",
    "min_song_session=song_sparkdf.groupby(\"userId\").min('count').sort('userId')\n",
    "min_song_session=min_song_session.withColumnRenamed(\"min(count)\",\"session_min_num_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg number of songs per session\n",
    "avg_song_session=song_sparkdf.groupby(\"userId\").avg('count').sort('userId')\n",
    "avg_song_session=avg_song_session.withColumnRenamed(\"avg(count)\",\"session_avg_num_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The times of song play for each song in user's listening list\n",
    "song_featuredf2=spark_df_clean.filter(spark_df_clean['page']==\"NextSong\").groupby(\"userId\",'song').count().sort(\"userId\")\n",
    "# number of different songs per user played\n",
    "diff_song_num=song_featuredf2.groupby(\"userId\").count().sort(\"userId\")\n",
    "diff_song_num=diff_song_num.withColumnRenamed(\"count\",\"diff_song_num\")\n",
    "# the most played songs per user played times\n",
    "most_freq_song_num=groupby(\"userId\").max('count').sort(\"userId\")\n",
    "most_freq_song_num=most_freq_song_num.withColumnRenamed(\"count\",\"most_freq_song_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature=features_merge(df_feature,max_song_session)\n",
    "df_feature=features_merge(df_feature,min_song_session)\n",
    "df_feature=features_merge(df_feature,avg_song_session)\n",
    "df_feature=features_merge(df_feature,diff_song_num)\n",
    "df_feature=features_merge(df_feature,most_freq_song_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
